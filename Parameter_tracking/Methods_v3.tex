\section{Methods}

\red{ What we are doing in this paper}
The Wendling model is capable of replicating key features observed in hippocampal EEG prior to, during and post seizure~\citep{wendling2002epileptic,wendling2005interictal}. Here the estimation of physiologically relevant model parameters from the Wendling model is considered. A UKF is used to estimate the model parameters of interest. For this study the estimation procedure is tested using EEG simulated from the Wendling model. By doing so it is possible to determine how robust the unscented Kalman filter is when estimating the Wendling model.

\subsection{Model Description and Simulation} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\red{Wendling model description.}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The Wendling model describes the aggregate membrane potentials and firing rates produced by different neural populations. Each neural population is either excited or inhibited by other populations in the model. The net effect of one population on another is determined by a scaling constant termed connectivity. A graphical representation of the model is shown in figure~\ref{fig: Biological}. In the model four different neural populations are considered. The pyramidal neural population is the generator of EEG. Excitatory interneurons excite the pyramidal neurons (this connection is often modelled as a time delayed recurrent connection of the pyramidal population. Slow and fast inhibitory interneurons suppress the pyramidal neural population. The pyramidal neural population excites the excitatory and slow and fast inhibitory populations. Lastly, slow inhibitory interneurons suppress the fast inhibitory neural population. The effect of each neural population on the other is scaled by connectivity. Connectivity accounts for the number of afferent synaptic connections between neural populations.  Lastly, a stochastic input to the model is added to account for the effect of afferent pyramidal neurons, from other areas, on the modeled neural mass.%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\centering
		\includegraphics{jpg/Biological_Model_Description1.png}
	\caption{Graphical Description of the Wendling model. Membrane potentials are shown and named $v_{b}$ where $b$ is $p$, $e$, $fi$ and $si$ for pyramidal, excitatory and slow and fast inhibitory populations, respectively. The synaptic gains of each population are specified by $\theta_{m}$ where $m$ is defined in the same manner as the membrane potentials. Each triangle and circle indicates a neural populations. In particular, the triangle shape indicates the pyramidal population and the circle shapes represent interneurons. The triangles and circles model the action of the soma for the neural populations, and convert membrane potentials to firing rates. The synapses convert the firing rates to membrane potentials. Lastly, each line indicates a neural connection, which is specified by a connectivity constant}
	\label{fig: Biological}
\end{figure}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\red{Mathematical functions used in the model} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The Wendling model consists of two primary structures. The first structure is a sigmoid function which converts an aggregate membrane potential to an average firing rate\begin{align}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{eqn: Sigmoid}
g(v(t)) &= \frac{g_{max}}{1+exp(r(v(t)-v_{0}))}. \end{align}  In equation~\ref{eqn: Sigmoid} $g(v(t))$ and $g_{max}$ are the firing rate and its maximum respectively, $r$ is the sigmoid gradient and $v_{0}$ is the membrane potential at which $0.5g_{max}$ is attained. To understand this sigmoid shape, conceptually it can be seen as the integral of a normal distribution. When considering a neural population there are numerous neurons which can be described by different sigmoid functions. The resulting sigmoid shape is a summation of the expected number of neurons that will fire given a specific membrane potential. %%%%%%%%%%%%%%%%%%%%%%%%%
The second structure is an average firing rate to aggregate membrane potential kernel \begin{align} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{eqn: Convert}
v_{b}(t) &= \theta_{b}(t)h_{b}(t)*g(v(t))\\
\label{eqn: Kernel} 
h_{b}(t) &= \frac{1}{\tau_{b}}t\exp\left(-\frac{t}{\tau_{b}}\right) & t \geq 0\\
h_{b}(t) & = 0 & t <0. \end{align} In equations~\ref{eqn: Convert}-\ref{eqn: Kernel} $v_{b}(t)$ is the aggregate membrane potential, $h_{b}(t)$ is a kernel which converts firing rates into membrane potentials, $\theta_{b}(t)$ is the synaptic gain and $\tau_{b}$ is the time constant. The function $h_{b}(t)$ is a delayed exponential decay function (Figure~\ref{fig: FR2PSP_final}). The subscript $b$ is used to indicate that each neural population is described with a different synaptic gain and time constant. The synaptic gains are described by $\theta_{b}(t)$ as they are the parameters that are altered in order to simulate seizure EEG (figure~\ref{fig: SeizureSim}). Further, the synaptic gains will also be the parameters that are estimated in this study. In this description of the Wendling model, numerous model parameters are considered to be stationary, including the maximum firing rate~($g_{max}$) and time constants~($\tau_{b}$). 
To solve the convolution in equation~\ref{eqn: Convert} both $g(v(t))$ and $h_{m}(t)$ are transformed into the Laplace domain \begin{align}%%%%%%%%%%%%%%%%%
\label{eqn: Laplace}
G(V(s)) &= \frac{g_{max}}{1+exp(r(V(S)-v_{0}))}\\
H_{b}(s) &= \frac{\mathrm{d}}{\mathrm{d}S}\left(-\frac{\theta_{b}(s)}{\tau_{b}}\left(\frac{1}{s+\frac{1}{\tau_{b}}}\right)\right)\\
H_{b}(s) &= \frac{\theta_{b}(s)}{\tau_{b}}\left(\frac{1}{s+\frac{1}{\tau_{b}}}\right)^2.
\end{align} It is clear here that the sigmoid function has no functional dependence on time, therefore the output of the sigmoid will be assumed to be an input described by $u_{b}(t)$, this applies to the model parameters $\theta_{b}(S)$ as well, this simplifies the convolution equation to \begin{align}%%%%%%%%%%%%%%%
G(V(s)) &= U_{b}(s)\\
H_{b}(s) &= \frac{\theta_{b}(s)}{\tau_{b}}\left(\frac{1}{s+\frac{1}{\tau_{b}}}\right)^2\\
\label{eqn: LaplaceNMM}
V_{b}(s) &= \frac{\theta_{b}(s)U_{b}(s)}{\tau_{b}}\left(\frac{1}{s+\frac{1}{\tau_{b}}}\right)^2.
\end{align} Using equation~\ref{eqn: LaplaceNMM} the membrane potential $v_{b}(t)$ can be described as a second order differential equation as follows \begin{align}%%%%%%%%%%%%%%%%%%%%%%%%%%
V_{b}(s) &= \frac{\theta_{b}(s)U_{b}(s)}{\tau_{b}}\left(\frac{1}{s^2+\frac{2s}{\tau_{b}}+\frac{1}{\tau^2_{b}}}\right)\\
\label{eqn: LaplaceDiff}
V_{b}(s)\left(s^2+\frac{2s}{\tau_{b}}+\frac{1}{\tau^2_{b}}\right) &= \frac{\theta_{b}(s)U_{b}(s)}{\tau_{b}}.
\end{align} Taking the inverse Laplace transform of equation~\ref{eqn: LaplaceDiff} results in \begin{align} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frac{\mathrm{d}}{\mathrm{d}t^2}v_{b}(t) + \frac{\mathrm{d}}{\mathrm{d}t}\left(\frac{2*v_{b}(t)}{\tau_{b}}\right) + \frac{v_{b}(t)}{\tau^2_{b}} &= \frac{\theta_{b}(t)u_{b}(t)}{\tau_{b}}\\
\label{eqn: DiffNMM}
\frac{\mathrm{d}}{\mathrm{d}t^2}v_{b}(t) &= \frac{\theta_{b}(t)u_{b}(t)}{\tau_{b}} - \frac{\mathrm{d}}{\mathrm{d}t}\left(\frac{2*v_{b}(t)}{\tau^2_{b}}\right) -\frac{v_{b}(t)}{\tau^2_{b}}. \end{align} Now we define a variable such that equation~\ref{eqn: DiffNMM} can be described as two differential equations \begin{align} %%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{eqn: dummy1}
\frac{\mathrm{d}}{\mathrm{d}t}v_{b}(t) &= z_{b}(t)\\
\label{eqn: dummy2}
\frac{\mathrm{d}}{\mathrm{d}t^2}v_{b}(t) &= \frac{\mathrm{d}}{\mathrm{d}t}z_{b}(t).\end{align} Here $z_{b}(t)$ is defined as the derivative of of the membrane potential $v_{b}(t)$. Lastly to simplify notation a time derivative operator is specified \begin{align}%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{eqn: DiffOp}
\frac{\mathrm{d}}{\mathrm{d}t}v_{b}(t) &= \dot{v}_{b}(t).
\end{align} %%%%%%%%%%%%%
\begin{figure}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\centering
		\includegraphics{pdf/FR2PSP.pdf}
	\caption{Impulse response of the firing rate to aggregate membrane potential function. The time constants and synaptic gains used for this figure correspond to that of background EEG~\citep{wendling2002epileptic}. The peaks of each response occur at time $\tau_{b}$ and the maximum membrane potential is $\exp(-1)\theta_{b}$, where $b$ is replaced by $p$, $si$ and $fi$ for excitatory, and slow and fast inhibitory time constants and gains, respectively. The inhibitory populations response is shown as negative, as it is their net effect on the system.}
	\label{fig: FR2PSP_final}
\end{figure} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\centering
		\includegraphics{../../Images/Images/pdf/InsetF1.pdf}
	\caption{Simulated seizure using the Wendling model. Here four different sets of synaptic gains are used to simulate the seizure. Four types of neural activity are demonstrated in this figure: background EEG (1-10s), interictal (11-20s), low voltage high frequency (21-30s) and seizure (31-40s). The last ten seconds shows background data again. Four insets are demonstrated in the figure each corresponding to a different type of activity. The insets in order from left to right show: background, interictal, low voltage high frequency and seizure activity.\red{A comparison to real seizure activity is shown in figure~\iref.}}
	\label{fig: SeizureSim}
\end{figure}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Equations~\ref{eqn: DiffNMM}-\ref{eqn: DiffOp} can be converted into a set of two differential equations \begin{align}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{eqn: FR2PSP1}
\dot{v}_{b}(t)&= z_{b}(t)\\
\label{eqn: FR2PSP2}
\dot{z}_{b}(t)&=\frac{\theta_{b}(t)}{\tau_{b}}n_{b}u_{b}(t)-2\frac{z_{b}(t)}{\tau_{b}}-\frac{v_{b}(t)}{\tau_{b}^{2}}.
\end{align} Here $v_{b}(t)$ is the average membrane potential and $z_{b}(t)$ is its derivative. $\theta_{b}(t)$ and $\tau_{b}$ are the specific neural populations synaptic gain and time constant. Lastly, $u_{b}(t)$ is the firing rate to the specific neural population considered and $n_{b}$ is a constant used to describe connectivity.

\red{Full mathematical description of the model}
Using equations~\ref{eqn: FR2PSP1}-\ref{eqn: FR2PSP2} the model can be described by a set of eight stochastic differential equations \begin{align}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\mathrm{d}v_{po}(t)&= z_{po}(t)\mathrm{d}t\\
\mathrm{d}z_{po}(t)&=\left(\frac{\theta_{p}(t)}{\tau_{p}}n_{p}g(v_{p}(t))-2\frac{z_{po}(t)}{\tau_{p}}-\frac{v_{po}(t)}{\tau_{p}^{2}}\right)\mathrm{d}t\\
\mathrm{d}v_{p1}(t)&= z_{p1}(t)\mathrm{d}t\\
\mathrm{d}z_{p1}(t)&=\left(\frac{\theta_{e}(t)}{\tau_{e}}(\mu +n_{e}g(v_{e}(t))-2\frac{z_{p1}(t)}{\tau_{e}}-\frac{v_{p1}(t)}{\tau_{e}^{2}}\right)\mathrm{d}t + \frac{\theta_{e}(t)}{\tau_{e}}\epsilon(t)\mathrm{d}W\\
\mathrm{d}v_{p2}(t)&= z_{p2}(t)\mathrm{d}t\\
\mathrm{d}z_{p2}(t)&=\left(\frac{\theta_{si}(t)}{\tau_{si}}n_{si}g(v_{si}(t))-2\frac{z_{p2}(t)}{\tau_{si}}-\frac{v_{p2}(t)}{\tau_{si}^{2}}\right)\mathrm{d}t\\
\mathrm{d}v_{p3}(t)&= z_{p3}(t)\mathrm{d}t\\
\mathrm{d}z_{p3}(t)&=\left(\frac{\theta_{fi}(t)}{\tau_{fi}}n_{fi}g(v_{fi}(t))-2\frac{z_{p3}(t)}{\tau_{fi}}-\frac{v_{p3}(t)}{\tau_{fi}^{2}}\right)\mathrm{d}t.
\end{align} I these equations $dW$ represents a Wiener process and is required as $\epsilon(t)\sim N(0,\sigma)$. Here $\sigma$ and $\mu$ describe the mean and variance of the stochastic model input. Further, $v_{po}(t) $ and $v_{p1-3}(t)$ represent the membrane potential produced by each neural population and $z_{po}(t) $ and $z_{p1-3}(t)$ their derivatives. $v_{b}(t) $ and $z_{b}(t) $ are the inputs to each neural population, and are considered to be the membrane potential of the specific population, where $b$ takes the values of $p$, $e$, $fi$ and $si$ representing pyramidal, excitatory, and slow and fast inhibitory populations, respectively. Therefore $v_{p}(t) $ is the output of the model. All $v_{b}(t) $ can be described in terms of $v_{po}(t)$ and $v_{p1-3}(t)$ as follows \begin{align}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
v_{p}(t) &= v_{p1}(t)-c_{4}v_{p2}(t)-v_{p3}(t)\\
v_{e}(t) &= c_{1}v_{po}(t)\\
v_{si}(t) &= c_{3}v_{po}(t)\\
v_{fi}(t) &= c_{5}v_{po}(t)-c_{6}v_{p2}(t),
\end{align} where $c_{1}$, $c_{3}$ and $c_{5}$ represent the connectivity strength from pyramidal to excitatory, slow inhibitory and fast inhibitory populations, respectively. The last two connectivity terms $c_{4}$ and $c_{6}$ represent the connectivity strength from the slow inhibitory to the excitatory and fast inhibitory populations, respectively. Lastly, all $n_{b}$ can be defined as connectivity constants where\begin{align}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
n_{p} &=1\\
n_{e} &=c_{2}\\
n_{si} &=c_{4}\\
n_{fi} &=c_{7}.
\end{align} Here $c_{2}$, $c_4$ and $c_7$ represent the connectivity strength from excitatory, slow inhibitory and fast inhibitory to the pyramidal population.

\red{Simulation of model}
This set of continuous stochastic differential equations is discretised using Euler-Mariyama's method, to simulate the artificial EEG \begin{align}
v_{po,k+1}&=v_{po,k}+Tz_{po,k}\\
z_{po,k+1}&=z_{po,k}+T\left(\frac{\theta_{p,k}}{\tau_{p}}n_{p}g(v_{p,k})-2\frac{z_{po,k}}{\tau_{p}}-\frac{v_{po,k}}{\tau_{p}^{2}}\right)\\
v_{p1,k+1}&=v_{p1,k}+Tz_{p1,k}\\
z_{p1,k+1}&=z_{p1,k}+T\left(\frac{\theta_{e,k}}{\tau_{e}}(\mu +n_{e}g(v_{e,k})-2\frac{z_{p1,k}}{\tau_{e}}-\frac{v_{p1,k}}{\tau_{e}^{2}}\right) + \sqrt{t}\frac{\theta_{e,k}}{\tau_{e}}\epsilon_{t}\\
v_{p2,k+1}&=v_{p2,k}+Tz_{p2,k}\\
z_{p2,k+1}&=z_{p2,k}+T\left(\frac{\theta_{si,k}}{\tau_{si}}n_{si}g(v_{si,k})-2\frac{z_{p2,k}}{\tau_{si}}-\frac{v_{p2,k}}{\tau_{si}^{2}}\right)\\
v_{p3,k+1}&=v_{p3,k}+Tz_{p3,k}\\
z_{p3,k+1}&=z_{p3,k}+T\left(\frac{\theta_{fi,k}}{\tau_{fi}}n_{fi}g(v_{fi,k})-2\frac{z_{p3,k}}{\tau_{fi}}-\frac{v_{p3,k}}{\tau_{fi}^{2}}\right).
\end{align} Where $k$ represents the current sample and $T$ is the period between samples. The static parameter values are demonstrated in  Tables~\ref{tab: Static}-\ref{tab: Connectivity}. The variance of the input ($\sigma$) is specified such that 99.7\% of realisations drawn from the Gaussian distribution fall within the specified maximum and minimum firing rate (table~\ref{tab: Static}). For the cases where the realisations from the Gaussian distribution are not contained within the limits specified, the specific sample of interest is redrawn from the same Gaussian distribution until the firing rate falls within the specified range. In Table~\ref{tab: Static} the parameters $\theta_{p,k}$, $\theta_{e,k}$, $\theta_{si,k}$ and $\theta_{fi,k}$ are not specified as these parameters will vary for different simulations. However, for this simulation it is assumed that \begin{align}
\theta_{p,k} = \theta_{e,k}.
\end{align} This assumption can be made as the excitatory population in this model specifies recurrent connections between pyramidal neurons.  
\singlespacing
\small
\begin{center}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\begin{table}
			\caption{Static Model Parameters~\citep{wendling2002epileptic}}
		\begin{tabular}{||p{4cm}|p{6cm}|p{1.5cm}|p{1.2cm}||}\hline
			 \textsc{Model parameter}  & \textsc{Physical description} & \textsc{Value} & \textsc{Units}  \\\hline\hline
			 $\tau_{p}$ & Time constant for pyramidal neurons & 100 & $s^{-1}$\\\hline
			 $\tau_{e}$ & Time constant for excitatory neurons & 100 & $s^{-1}$\\\hline
			 $\tau_{si}$ & Time constant for slow inhibitory neurons & 35 & $s^{-1}$\\\hline
			 $\tau_{fi}$ & Time constant for fast inhibitory neurons & 500 & $s^{-1}$\\\hline
			 $c$ & Connectivity constant & 135 & NA\\\hline
			 $g_{max}$ & Maximum firing rate & 5 & Hz \\\hline
			 $v_{0}$ & PSP for which 50\% firing rate is achieved & 6 & $mV^{-1}$\\\hline
			 $r$ & Gradient of sigmoid function & 0.56 & NA \\\hline
			 $f_{max}$ & Maximum input firing rate & 150 & Hz \\\hline
			 $f_{min}$ & Minimum input firing rate & 30 & Hz \\\hline
			 $\mu$ & Input mean firing rate & 90 & $Hz$\\\hline
			 $\sigma$ & Variance of input firing rate & 15 & $Hz$\\\hline\hline 
		\end{tabular}
		\label{tab: Static}
	\end{table}
\end{center}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{center}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\begin{table}
			\caption[Static Model Parameters: Connectivity]{Static model parameters: Connectivity~\citep{wendling2002epileptic}. Here $c$ is the connectivity constant specified in Table~\ref{tab: Static}. Terms in brackets indicate the direction in which the constant affects the system. Here P, E, SI and FI represent populations of pyramidal neurons and excitatory, slow inhibitory and fast inhibitory interneurons, respectively.}
		\begin{tabular}{||p{4cm}|p{7cm}|p{2cm}||}\hline
			 \textsc{Model parameter}  & \textsc{Physical description} & \textsc{Value}
			   \\\hline\hline
			 $c_{1}$ & Connectivity constant (P - E) & $c$ \\\hline
			 $c_{2}$ & Connectivity constant (E + I - P) & $0.8c$ \\\hline
			 $c_{3}$ & Connectivity constant (P - SI) & $0.25c$  \\\hline
			 $c_{4}$ & Connectivity constant (SI - P)& $0.25c$ \\\hline
			 $c_{5}$ & Connectivity constant (P - FI) & $0.3c$ \\\hline
			 $c_{6}$ & Connectivity constant (SI - FI) & $0.1c$ \\\hline
			 $c_{7}$ & Connectivity constant (SI - P) & $0.8c$ \\\hline\hline
		\end{tabular}
		\label{tab: Connectivity}
	\end{table}
\end{center}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\normalsize
\onehalfspacing

\subsection{Estimation}

\red{Generic description on a nonlinear system}
To begin a generic nonlinear system is defined where \begin{align}
\label{eqn: NonlinEstS}
\mathbf{\dot{x}}(t) &= \mathbf{A}(\mathbf{x}(t),\mathbf{\theta}(t)) + \mathbf{B}(\mathbf{u}(t)) + \mathbf{n}(t)\\
\label{eqn: NonlinEstO}
\mathbf{y}(t)  &= \mathbf{C}(\mathbf{x}(t)) +\mathbf{D}(\mathbf{u}(t))+\mathbf{r}(t),
\end{align} where boldface indicates a matrix. Here $\mathbf{x}(t)$ is the state vector and $\dot{\mathbf{x}}(t)$ its derivative where
\[ \mathbf{\dot{x}}(t) = \left[ \begin{array}[pos]{c}
\dot{x}_{1}(t)\\
\vdots \\
\dot{x}_{n}(t) \end{array} \right] .\]  In equations~\ref{eqn: NonlinEstS}-\ref{eqn: NonlinEstO} $\mathbf{A}$, $\mathbf{B}$, $\mathbf{C}$ and $\mathbf{D}$ are the state, input, output and input to output functions and $\mathbf{u}(t)$ the input to the model. The output of the model is $\mathbf{y}(t)$ with $\mathbf{n}(t)$ the model noise, or uncertainty and $\mathbf{r}(t)$ the measurement error, or observation noise. Here $\mathbf{n}(t)$ takes into account that the model is not a perfect descriptor of the particular region of the brain, and $\mathbf{r}(t)$ describes the maximum amplitude of the noise in the observations. Both $\mathbf{n}(t)$ and $\mathbf{r}(t)$ are zero mean Gaussian distributions with a system dependant variance. It is important to note that this assumption of a Gaussian distribution is only valid when the number of samples for the estimation procedure is large. In other words, the sampling rate needs to be much greater than the maximum frequency that the model dynamics describes. This is often refered to as an oversampled system.

\red{Introduction to the UKF}
Estimation is usually performed to estimate the model states $\mathbf{x}(t)$ given some observation $\mathbf{y}(t)$ . One method that is often use for estimating linear systems is the Kalman filter. The Kalman filter consists of two steps: prediction and correction. In the prediction step model states are propagated through the system and are used to determine the expected value of the states at the next time step. Using a first order Euler-Maruyama method this can be described by \begin{align}
\label{eqn: StateProgL}
\mathbf{x}_{k+1}^{-} &= \mathbf{x}_{k} + T(\mathbf{A}\mathbf{x}_{k} +\mathbf{B}\mathbf{u}_{k})+\sqrt{T}\mathbf{n}_{k}\\
\label{eqn: YProp}
\mathbf{y}_{k+1}^{-}  &= \mathbf{y}_{k} + T(\mathbf{C}\mathbf{y}_{k}+\mathbf{D}\mathbf{u}_{k}) +\sqrt{T}\mathbf{r}_{k}
\end{align} where the subscript in $\mathbf{x}_{k}^{-}$ is used to indicate the current sample and $T$ is the sampling period. The stochastic variables $\mathbf{n}_{k}\sim N(0,\mathbf{\sigma}_{1}$ and $\mathbf{r}_{k}\sim N(0,\mathbf{\sigma}_{2}$ where $\mathbf{\sigma}_{1}$ and $\mathbf{\sigma}_2$ are the standard deviations for each respective noise process. Here $N(\cdot)$ is a Gaussian or normal distribution. The superscript in $\mathbf{x}_{k}^{-}$ is used to indicate that this estimate is a prediction that has not yet been corrected by the current observation. Performing this kind of prediction for a nonlinear system would be inaccurate. The reason for this is that propagation of states like this in a system would require the assumption that the maximum error in the states remains constant for all time. However, in a nonlinear system this is not true, as states error can vary from one prediction to the next. In other words if the original state estimate is incorrect in a nonlinear system it is possible that the error can increase from one step to the next. This is not the case for linear system. In order to account for this error, or the altering of state covariance, an unscented filter is used in the prediction step for nonlinear systems. The advantage of an unscented filter over local linearisation techniques is both speed is improved and discontinuities can be handled. 

\red{The unscented filter}
The unscented filter is completely described by the states mean and covariance such that sigma points can be drawn as follows\begin{align}
\label{eqn: Unscented_Transform1}
\mathbf{\mathcal{X}}_{n} &= \mathbf{\overline{x}}_{k} + (\sqrt{\kappa+D_{x}\mathbf{P_{xx,k}}})_{n} \quad n=1,\hdots,D_x\\
\label{eqn: Unscented_Transform2}
\mathbf{\mathcal{X}}_{n+D_{x}} &= \mathbf{\overline{x}}_{k} - (\sqrt{\kappa+D_{x}\mathbf{P_{xx,k}}})_{n} \quad n=1,\hdots,D_x,\\
\label{eqn: Unscented_TransformY1}
\mathbf{\mathcal{Y}}_{n} &= \mathbf{\overline{y}}_{k} + (\sqrt{\kappa+D_{y}\mathbf{P_{yy,k}}})_{n} \quad n=1,\hdots,D_y\\
\label{eqn: Unscented_TransformY2}
\mathbf{\mathcal{Y}}_{n+D_{y}} &= \mathbf{\overline{y}}_{k} - (\sqrt{\kappa+D_{y}\mathbf{P_{yy,k}}})_{n} \quad n=1,\hdots,D_y,
\end{align} where $\mathbf{\overline{x}}_{k}$ and $\mathbf{\overline{y}}_{k}$ are the current state estimate and observation. $\sqrt{\cdot}_{n}$ denotes the $n$th row of the matrix square root. Here $D_{x}$ and $D_{y}$ indicate the number of states in the system and the number of observations. Covariance matrices $\mathbf{P_{xx,k}}$ and $\mathbf{P_{yy,k}}$ indicated the expected error of the current state and observation estimate~\red{explain observation estimate}. The points $\mathbf{\mathcal{X}}$ are called sigma points and represent the states one standard deviation away for the estimated mean. The term $\kappa$ is a predefined constant which determine the relative effect of the propagation of the mean. If $\kappa$ is equal to zero than the system mean is not propagated as a sigma point. However, if $\kappa$ is greater than zero then \begin{align}
\mathbf{\mathcal{X}}_{0} &= \mathbf{\overline{x}}_{k}.
\end{align} Now their are 2$D_{x}$ sigma points when $\kappa$ is zero or 2$D_{x}$+1 sigma points when it is greater than zero. The sigma points are propagated through the system in oder to update the expectation about the state mean and error. \begin{align}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\mathbf{\mathcal{X}}_{n,k+1} &= \mathbf{\mathcal{X}}_{n,k}+ T(\mathbf{A}(\mathbf{\mathcal{X}}_{n,k}) +\mathbf{B}(\mathbf{u}_{k})) +\sqrt{T}{n}_{k}\\
\overline{\mathbf{x}}_{k+1}^{-} &= \frac{1}{2D_{x}+\kappa}\sum_{n=1}^{2D_{x}} \mathbf{\mathcal{X}}_{n,k+1}\\
\mathbf{P}_{xx,k+1}^{-} &= \frac{1}{2D_{x}+\kappa}\sum_{n=1}^{2D_{x}} (\mathbf{\mathcal{X}}_{n,k+1} -\mathbf{\overline{x}}_{k+1}^{-})(\mathbf{\mathcal{X}}_{n,k+1}-\mathbf{\overline{x}}_{k+1}^{-})^{\top} + \mathbf{Q}.%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{align} Here $\overline{\mathbf{x}}_{k+1}^{-}$ and $\mathbf{P}_{xx,k+1}^{-}$ are the predictions for the state and state covariance matrices. The superscript $a^{-}$ is used to indicate an uncorrected prediction. The term $\mathbf{Q}$ is the expectation of the model error $n_{k}$. It is now possible to make a prediction about the observation at sample $k+1$ by propagating the sigma points through equation~\ref{eqn: YProp} \begin{align} %%%%%%%%%%%%%%%%%%%%%%%%%%%%
\mathbf{\mathcal{Y}}_{n,k+1} &= \mathbf{\mathcal{Y}}_{n,k} + T(\mathbf{C}(\mathbf{\mathcal{X}}_{n,k+1})+ \mathbf{D}(\mathbf{u}_{k}))+ \sqrt{T}{r}_{k}\\
\overline{\mathbf{y}}_{k+1}^{-} &= \frac{1}{2D_{x}+\kappa}\sum_{n=1}^{2D_{x}} \mathbf{\mathcal{Y}}_{n,k+1}\\
\label{eqn: statecovg}
\mathbf{P}_{xy,k+1}^{-} &= \frac{1}{2D_{x}+\kappa}\sum_{n=1}^{2D_{x}} (\mathbf{\mathcal{X}}_{n,k+1}-\overline{\mathbf{x}}_{n,k+1}) (\mathbf{\mathcal{Y}}_{n,k+1}-\overline{\mathbf{y}}_{k+1}^{-})^{\top}\\
\mathbf{P}_{yy,k+1}^{-} &= \frac{1}{2D_{x}+\kappa}\sum_{n=1}^{2D_{x}} (\mathbf{\mathcal{Y}}_{n,k+1}-\overline{\mathbf{y}}_{k+1}^{-}) (\mathbf{\mathcal{Y}}_{n,k+1}-\overline{\mathbf{y}}_{k+1}^{-})^{\top} +\mathbf{R},%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{align} where $\overline{\mathbf{y}}_{k+1}^{-}$ and $\mathbf{P}_{yy,k+1}^{-}$ are the predictions for the model output and its covariance. $\mathbf{P}_{xy,k+1}^{-}$ is the covariance matrix of the states and observations. Lastly, $\mathbf{R}$ is the expectation of the observation error $r_{k}$.

\red{How states are predicted using the unscented transform}
The predictions of the states ($\overline{\mathbf{x}}_{k+1}^{-}$) and observations ($\overline{\mathbf{y}}_{k+1}^{-}$) now need to be corrected based on the observations. This is achieved by \begin{align}
\mathbf{K} &= \mathbf{P}_{xy,k+1}^{-}(\mathbf{P}_{yy,k+1}^{-})^{-1}\\
\overline{\mathbf{x}}_{k+1} &= \overline{\mathbf{x}}_{k+1}^{-} + \mathbf{K}(\mathbf{y}_{k+1}-\overline{\mathbf{y}}_{k+1}^{-})\\
\mathbf{P}_{xx,k+1} &= \mathbf{P}_{xx,k+1}^{-} - \mathbf{K}(\mathbf{P}_{xy,k+1}^{-})^{\top},]
\end{align} where $\mathbf{y}_{k}$ is the recording or observation made, $\overline{\mathbf{x}}_{k+1}$ is the corrected estimate of the state and $\mathbf{P}_{xx,k+1}$ is the estimate of its error. This set of equations describes the UKF and how it can be used to estimate states. However, for this study estimation of fast states (membrane potentials and their derivatives) and parameters is required (dual estimation).

\red{Definition of slow state matrix and its dynamics}
Firstly, reconsider the extended neural mass model. The parameters that are being estimated are $\theta_{p,k}$, $\theta_{e,k}$, $\theta_{si,k}$ and $\theta_{fi,k}$. Here it is assumed that \begin{align}
\theta_{p,k} = \theta_{e,k}.
\end{align} Therefore, three parameters need to be estimated, and a parameter matrix is defined
\[ \mathbf{\theta}_{k} = \left[ \begin{array}[pos]{c}
\theta_{p,k}\\
\theta_{si,k} \\
\theta_{fi,k} \end{array} \right] .\] This parameter matrix is then augmented to the original state matrix
\[ \mathbf{x}_{k} = \left[ \begin{array}[pos]{c}
\mathbf{x}_{k}\\
\mathbf{\theta}_{k} \end{array} \right] .\] The change in model parameters occurs at a longer time scale than the model states. Therefore, for convenience, the model states and parameters are referred to as fast and slow states. Therefore, the final state matrix consists of fast and slow states. The next issue to consider is the description of the dynamics for the slow states. Due to the prediction correction steps of the unscented Kalman filter these slow states can be assigned trivial dynamics such that
\begin{align}
\label{eqn: parameterdynamics}
\mathbf{\theta}_{k+1} &= \mathbf{\theta}_{k} + \mathbf{\eta_{k}}\\
E(\mathbf{\theta}_{k+1}) &= \mathbf{\theta}_{k}\\
P_{\mathbf{\theta} \mathbf{\theta},k+1} &= \mathbf{\Psi},
\end{align} where $E(\cdot)$ is the expectation function and $\mathbf{\eta}_{k} \sim N(0,\mathbf{\sigma})$. Where $N(\cdot)$ is a normal distribution with mean zero and variance $\sigma$.

\red{Intialisation of UKF for stationary parameters}
When initialising the unscented Kalman filter the uncertainty and standard deviation of the model needs to be specified. Initial estimations are performed under the assumption that the model's slow states are stationary. This assumption allows the uncertainty in slow states to be minimal. This uncertainty characterises possible model inaccuracy, and also allows the slow state to slowly vary until they converges to their true values. Standard deviations in the estimation description are described such that one standard deviation from the midpoint of the specified slow states range encompasses all possible values for the particular state. Further, the uncertainty of states directly affected by the stochastic model input is increased to account for the unpredictable nature of this signal when simulating. This is required as a stochastic process cannot be estimated accurately. 

Due to the stochastic model input it is non-trivial to determine the maximum limit of the physiological range describing the fast states. Therefore, numerous simulations are performed and the resulting mean and standard deviation for each states is used as the initial mean and standard deviation. Therefore, \begin{align}
v_{b,0} = \frac{1}{n}\sum\limits_{i=1}^n E(v_{b,k}), \end{align} where $b$ indicates the number of simulation performed and $v_{m,0}$ and $E(v_{m,k})$ indicate the expected value of the initialised fast states and the expected value of each simulation, respectively. For the standard deviation the average over multiple simulations for a normal distribution can be determined by \begin{align}
\sigma^2_{b,0} = \frac{1}{n-1}\sum\limits_{i=1}^n \sigma^2_{b,k},\end{align} where $\sigma^2_{b,0}$ and $\sigma^2_{b,k}$ are the variance of the initial guess and of each states simulation results, respectively.


Next the assignment of the mean and covariance of the slow states is considered. For this calculation it is assumed that all possible values of the slow states are equally probable. The range of these slow states is infinite; however, their is a physiological bound on their values described in the original description of the model~\citep{wendling2002epileptic}. These bounds will be used and are defined as $\theta_{b,max}$ and $\theta_{b,min}$. The mean and covariance of the slow states is therefore\begin{align}
\label{eqn: InitThetaMean}
\overline{\theta}_{b,0} = \frac{\theta_{b,max}+\theta_{b,min}}{2}\\
\label{eqn: InitThetaCoV}
P_{\theta\theta,0} = (\overline{\theta}_{b,0}-\theta_{b,min})^2.
\end{align}

\red{Intialisation of UKF for varying parameters}
When estimating slow states that vary within a single simulation the uncertainty assigned to these parameters is increased. This increase in uncertainty guarantees that the estimation procedure will not converge to a specific slow state and remain there, but instead will track them as they vary. 

\red{Estimation of model input mean}
Lastly, estimation of the stochastic input's mean to the neural mass is considered. Here it is assumed that the model input mean can be assumed to be varying slowly, similarly to the model's slow states. By doing so the input mean can be augmented to the state matrix and assigned trivial dynamics. Therefore the initial mean and standard deviation of the model input can be defined in a similar manner to the slow states. With the physiological bounds on the input specific by~\cite{wendling2002epileptic} the mean and standard deviation of the input can be defined by equations~\ref{eqn: InitThetaMean}-\ref{eqn: InitThetaCoV}.

\red{Robustness test}
In the results section the performance of the estimation procedure is determined. Initially, only estimation of fast states is considered. This is then developed to the full estimation procedure where all model parameters and the input mean are estimated. The estimation procedure is then tested under numerous observation noise conditions, and with varying levels of error in the initialisation of states.
